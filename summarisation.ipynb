{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from wandb.integration.langchain import WandbTracer\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, AgentExecutor, BaseSingleActionAgent, AgentType\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "os.chdir(\"/notebooks/learn-langchain\")\n",
    "from langchain_app.models.text_generation_web_ui import (\n",
    "    build_text_generation_web_ui_client_llm,\n",
    ")\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.callbacks import WandbCallbackHandler, StdOutCallbackHandler\n",
    "\n",
    "# Set Wandb API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"9ae105f6bbe37ca6eff03ea9c3af7df398713e7e\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract JSON summarise the article and save results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\n",
    "wandb_callback = WandbTracer({\"project\": \"wandb_prompts_quickstart\"}\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), wandb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'Visa Business News', 'title': 'Acceptance', 'geo': 'Germany', 'audience': 'Sales', 'publication_date': '2023-05-04T00:00:00Z'}\n"
     ]
    }
   ],
   "source": [
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "json_string = '''```json\n",
    "{\n",
    "   \"category\":\"Visa Business News\",\n",
    "   \"title\":\"Acceptance\",\n",
    "   \"geo\":\"Germany\",\n",
    "   \"audience\":\"Sales\",\n",
    "   \"publication_date\":\"2023-05-04T00:00:00Z\"\n",
    "}\n",
    "```'''\n",
    "json_object = clean_and_load_json(json_string)\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "    ### Instruction: \n",
      "    Extract the following information from the text in input.\n",
      "    1. Category\n",
      "    2. Title\n",
      "    3. GEO\n",
      "    4. Audience\n",
      "    5. Publication date\n",
      "\n",
      "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
      "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
      "    As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}} the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema.\n",
      "    The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "    Here is the output schema: {\"type\" : \"object\", \"properties\" : {\"category\" : {\"type\" : \"string\"}, {\"title\" : {\"type\" : \"string\"}, {\"geo\" : {\"type\" : \"string\"}, {\"audience\" : {\"type\" : \"string\"}, {\"publication_date\" : {\"type\" : \"date\"} } } } }\n",
      "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
      "\n",
      "    ### Input: \n",
      "    Visa Business News \n",
      "\n",
      "Acceptance \n",
      "\n",
      "4 May 2023 \n",
      "\n",
      "Fourth “Acquirer Rallye” Sales Contest Will Be Held in Germany \n",
      "Europe | Acquirers \n",
      "Visa Network; V PAY\n",
      "\n",
      "    ### Response:\n",
      "    JSON:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/notebooks/files/AI12980-Fourth ‘Acquirer Rallye’ Sales Contest Will Be Held in Germany.pdf\n",
      "\n",
      "```json\n",
      "{\n",
      "   \"category\":\"Visa Business News\",\n",
      "   \"title\":\"Acceptance\",\n",
      "   \"geo\":\"Germany\",\n",
      "   \"audience\":\"Sales\",\n",
      "   \"publication_date\":\"2023-05-04T00:00:00Z\"\n",
      "}\n",
      "``` \n",
      "    {\n",
      "      \"article-id\": \"AI12980\",\n",
      "      \"effective-date\": \"2023-05-15T00:00:00Z\",\n",
      "      \"mark-your-calendar-date\": \"2023-08-19T00:00:00Z\"\n",
      "    }\n",
      "  article-id        effective-date mark-your-calendar-date  \\\n",
      "0    AI12980  2023-05-15T00:00:00Z    2023-08-19T00:00:00Z   \n",
      "\n",
      "             category       title      geo audience      publication_date  \\\n",
      "0  Visa Business News  Acceptance  Germany    Sales  2023-05-04T00:00:00Z   \n",
      "\n",
      "                                      medium_summary  \n",
      "0   \\n\\nVisa is holding a contest called the \"Fou...  \n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "    ### Instruction: \n",
      "    Extract the following information from the text in input.\n",
      "    1. Category\n",
      "    2. Title\n",
      "    3. GEO\n",
      "    4. Audience\n",
      "    5. Publication date\n",
      "\n",
      "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
      "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
      "    As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}} the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema.\n",
      "    The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "    Here is the output schema: {\"type\" : \"object\", \"properties\" : {\"category\" : {\"type\" : \"string\"}, {\"title\" : {\"type\" : \"string\"}, {\"geo\" : {\"type\" : \"string\"}, {\"audience\" : {\"type\" : \"string\"}, {\"publication_date\" : {\"type\" : \"date\"} } } } }\n",
      "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
      "\n",
      "    ### Input: \n",
      "    Business Enhancements \n",
      "\n",
      "3 May 2023 \n",
      "\n",
      "Changes to the TC 33.A Acquirer Capture File Further Modified in Global \n",
      "Technical Letter \n",
      "Global | Acquirers, Issuers, Processors, Software Vendors \n",
      "Visa Network; V PAY; Europe Processing\n",
      "\n",
      "    ### Response:\n",
      "    JSON:\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/notebooks/files/AI13048-Changes to the TC 33.A Acquirer Capture File Further Modified in Global Technical Letter.pdf\n",
      "\n",
      "{\n",
      "   \"category\":\"Business Enhancements\",\n",
      "   \"title\":\"Changes to the TC 33.A Acquirer Capture File Further Modified in Global Technical Letter\",\n",
      "   \"geo\":\"Europe\",\n",
      "   \"audience\":\"Acquirers, Issuers, Processors, Software Vendors\",\n",
      "   \"publication_date\":\"2023-05-03T00:0 \n",
      "{\n",
      "  \"article-id\": \"AI13048\",\n",
      "  \"effective-date\": \"2023-05-03T00:00:00Z\",\n",
      "  \"mark-your-calendar-date\": \"2023-06-05T00:00:00Z\"\n",
      "}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't decode the cleaned string as JSON: \n{\n   \"category\":\"Business Enhancements\",\n   \"title\":\"Changes to the TC 33.A Acquirer Capture File Further Modified in Global Technical Letter\",\n   \"geo\":\"Europe\",\n   \"audience\":\"Acquirers, Issuers, Processors, Software Vendors\",\n   \"publication_date\":\"2023-05-03T00:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 107\u001b[0m, in \u001b[0;36mclean_and_load_json\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mloads(cleaned_json_string)\n\u001b[1;32m    108\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/envs/textgen/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 7 column 23 (char 252)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 233\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mprint\u001b[39m(json_5, json_3)\n\u001b[1;32m    232\u001b[0m json_3 \u001b[39m=\u001b[39m clean_and_load_json(json_3)\n\u001b[0;32m--> 233\u001b[0m json_5 \u001b[39m=\u001b[39m clean_and_load_json(json_5)\n\u001b[1;32m    234\u001b[0m \u001b[39m# Combine two JSONs into one\u001b[39;00m\n\u001b[1;32m    235\u001b[0m combined_json \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mjson_3, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mjson_5}\n",
      "Cell \u001b[0;32mIn[25], line 109\u001b[0m, in \u001b[0;36mclean_and_load_json\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(cleaned_json_string)\n\u001b[1;32m    108\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt decode the cleaned string as JSON: \u001b[39m\u001b[39m{\u001b[39;00mcleaned_json_string\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't decode the cleaned string as JSON: \n{\n   \"category\":\"Business Enhancements\",\n   \"title\":\"Changes to the TC 33.A Acquirer Capture File Further Modified in Global Technical Letter\",\n   \"geo\":\"Europe\",\n   \"audience\":\"Acquirers, Issuers, Processors, Software Vendors\",\n   \"publication_date\":\"2023-05-03T00:0"
     ]
    }
   ],
   "source": [
    "# Create an instance of LLM for generating a JSON extract\n",
    "llm_extract_json = build_text_generation_web_ui_client_llm(\n",
    "    parameters={\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "}\n",
    ")\n",
    "\n",
    "# Create an instance of LLM for generating a summary of 200 tokens - Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.01,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 5 fields\n",
    "text_extract_json_5 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 3 fields\n",
    "text_extract_json_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract text for summary\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_filepath = '/notebooks/files/extracted_data.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(csv_filepath):\n",
    "    # If it exists, load it into a DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "else:\n",
    "    # If it doesn't exist, initialize an empty DataFrame with the necessary columns\n",
    "    df = pd.DataFrame(columns=[\n",
    "    \"effective-date\", \n",
    "    \"mark-your-calendar-date\", \n",
    "    \"article-id\", \n",
    "    \"category\", \n",
    "    \"title\", \n",
    "    \"geo\", \n",
    "    \"audience\", \n",
    "    \"publication_date\",\n",
    "    \"medium_summary\"\n",
    "    ])\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Define a function to sanitize a file name\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "\n",
    "# Get the first PDF file path from the list\n",
    "for file_path in pdf_files:\n",
    "\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 3 fields\n",
    "    \n",
    "    template_json_3 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input:\n",
    "    1. Article ID\n",
    "    2. Effective date\n",
    "    3. Mark your calendar date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output3_1} the object {output3_2} is a well-formatted instance of the schema.\n",
    "    The object {output3_3} is not well-formatted.\n",
    "    Here is the output schema: {output3_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 5 fields\n",
    "    template_json_5 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input.\n",
    "    1. Category\n",
    "    2. Title\n",
    "    3. GEO\n",
    "    4. Audience\n",
    "    5. Publication date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output5_1} the object {output5_2} is a well-formatted instance of the schema.\n",
    "    The object {output5_3} is not well-formatted.\n",
    "    Here is the output schema: {output5_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sub-templates for the chain\n",
    "    output3_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output3_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output3_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output3_4 = '{\"type\" : \"object\", \"properties\" : {\"article-id\" : {\"type\" : \"string\"}, {\"effective-date\" : {\"type\" : \"date\"}, {\"mark-your-calendar-date\" : {\"type\" : \"date\"} } } } }'\n",
    "    output5_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output5_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output5_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output5_4 = '{\"type\" : \"object\", \"properties\" : {\"category\" : {\"type\" : \"string\"}, {\"title\" : {\"type\" : \"string\"}, {\"geo\" : {\"type\" : \"string\"}, {\"audience\" : {\"type\" : \"string\"}, {\"publication_date\" : {\"type\" : \"date\"} } } } }'\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt_json_5 = PromptTemplate(template=template_json_5, input_variables=['question', \n",
    "                                                                              'output5_1', \n",
    "                                                                              'output5_2',\n",
    "                                                                              'output5_3',\n",
    "                                                                              'output5_4'\n",
    "                                                                              ])\n",
    "    prompt_json_3 = PromptTemplate(template=template_json_3, input_variables=['question', \n",
    "                                                                              'output3_1', \n",
    "                                                                              'output3_2',\n",
    "                                                                              'output3_3',\n",
    "                                                                              'output3_4'\n",
    "                                                                              ])\n",
    "\n",
    "    # Create a chain for generating a JSON extract\n",
    "    llm_chain_json_5 = LLMChain(llm=llm_extract_json, prompt=prompt_json_5, callbacks=callbacks)\n",
    "    llm_chain_json_3 = LLMChain(llm=llm_extract_json, prompt=prompt_json_3)\n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 5 fields\n",
    "    docs_json_5 = text_extract_json_5.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_5[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 5 fields and load resulting JSON into a variable\n",
    "    json_5 = llm_chain_json_5.run({'question': question, \n",
    "                                   'output5_1': output5_1, \n",
    "                                   'output5_2': output5_2,\n",
    "                                   'output5_3': output5_3,\n",
    "                                   'output5_4': output5_4\n",
    "                                   })\n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 3 fields\n",
    "    docs_json_3 = text_extract_json_3.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_3[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 3 fields and load resulting JSON into a variable\n",
    "    json_3 = llm_chain_json_3.run({'question': question, \n",
    "                                   'output3_1': output3_1, \n",
    "                                   'output3_2': output3_2,\n",
    "                                   'output3_3': output3_3,\n",
    "                                   'output3_4': output3_4\n",
    "                                  })\n",
    "    \n",
    "    # Print the file path\n",
    "    print(file_path)\n",
    "\n",
    "    # Print the results\n",
    "    print(json_5, json_3)\n",
    "\n",
    "    json_3 = clean_and_load_json(json_3)\n",
    "    json_5 = clean_and_load_json(json_5)\n",
    "    # Combine two JSONs into one\n",
    "    combined_json = {**json_3, **json_5}\n",
    "\n",
    "    # Split text into chunks for summary generation\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary and load resulting summary into a variable\n",
    "    med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Check if the summary is None and replace it with an empty string\n",
    "    if med_sum is None:\n",
    "        med_sum = ''\n",
    "\n",
    "    # Convert the summary to a string\n",
    "    med_sum = str(med_sum)\n",
    "\n",
    "    # Create a new row for the DataFrame\n",
    "    new_row = pd.DataFrame.from_records([combined_json])\n",
    "    new_row['medium_summary'] = [med_sum]\n",
    "\n",
    "    # Add a new row to the existing DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('/notebooks/files/extracted_data.csv', index=False)\n",
    "\n",
    "    def rename_file(df, file_path, fields):\n",
    "        # Get the last row of the DataFrame\n",
    "        row = df.iloc[-1]\n",
    "        \n",
    "        # Initialize an empty list for the new file name components\n",
    "        file_name_components = []\n",
    "        \n",
    "        # For each field...\n",
    "        for field in fields:\n",
    "            # If the field value is not NaN...\n",
    "            if not pd.isna(row[field]):\n",
    "                # Clean the field value and append it to the file name components\n",
    "                file_name_components.append(sanitize_filename(str(row[field])))\n",
    "\n",
    "        # If there are no valid file name components, use the original file name\n",
    "        if not file_name_components:\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            file_name_components.append(base_name)\n",
    "                \n",
    "        # Join the file name components with a dash and add the file extension\n",
    "        new_file_name = '-'.join(file_name_components) + '._pdf'\n",
    "    \n",
    "        # Rename the file\n",
    "        os.rename(file_path, os.path.join(os.path.dirname(file_path), new_file_name))\n",
    "\n",
    "    # Call the function to rename the file at the end of processing each file\n",
    "    rename_file(df, file_path, ['article-id', 'title'])\n",
    "\n",
    "    # Print the new row of a DataFrame\n",
    "    print(new_row)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different parameters of the model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/test/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Create an instance of LLM for generating a summary based on a 200 token text Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.18,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter for summary generation\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# Create an empty list to save the results\n",
    "results = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Load PDF file\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #  1. GENERATE MEDIUM SIZE SUMMARY\n",
    "\n",
    "    # Split text into chunks of 2500 characters\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Save results to list\n",
    "    results.append(docs_med_sum)\n",
    "\n",
    "# Print all results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Суммаризация PDF файлов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Суммаризация текста и сохранение результатов в текстовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем путь к папке с PDF-файлами\n",
    "input_directory = \"/notebooks/files\"\n",
    "\n",
    "# Получаем список PDF-файлов\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Создаем инстанс LLM для генерации summary на основе текста размером 200 токенов Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Создаем инстанс разделителя текста на 2500 символов с перекрытием в 0 символов (для генерации summary)\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Создаем цепочку для генерации summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Загружаем PDF-файл\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #1. ГЕНЕРАЦИЯ MEDIAN SIZE SUMMARY\n",
    "\n",
    "    #Разрезаем текст на куски по 2500 символов \n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "    # Отправляем документы в обработку для генерации summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Заменяем расширение файла на .txt и сохраняем результат\n",
    "    output_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        output_file.write(docs_med_sum)\n",
    "\n",
    "    print(f\"Обработан файл: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем PDF документ и делаем из него текстовый файл с помощью langchain.document_loaders.PDFMinerLoader\n",
    "file_path = '/notebooks/files/AI12944 - Updates to Fraud and Consumer Dispute Rules.pdf'\n",
    "loader = PDFMinerLoader(file_path)\n",
    "document = loader.load()\n",
    "\n",
    "text = document[0].page_content\n",
    "\n",
    "llm = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.001,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 1,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "    })\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose = True)\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
