{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "# import wandb\n",
    "from datetime import datetime\n",
    "# from wandb.integration.langchain import WandbTracer\n",
    "from langchain.callbacks import ClearMLCallbackHandler\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, AgentExecutor, BaseSingleActionAgent, AgentType\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "os.chdir(\"/notebooks/learn-langchain\")\n",
    "from langchain_app.models.text_generation_web_ui import (\n",
    "    build_text_generation_web_ui_client_llm,\n",
    ")\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# from langchain.callbacks import WandbCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# Set Wandb API key\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"9ae105f6bbe37ca6eff03ea9c3af7df398713e7e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=R2Q12RB4XKJN6G14YJC4\n",
    "%env CLEARML_API_SECRET_KEY=hfYxpPgSUUm4dRfiH0HsJCARjQsSWNfVkjPgyel9u2HS7ShYwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearml_callback = ClearMLCallbackHandler(\n",
    "    task_type=\"inference\",\n",
    "    project_name=\"langchain_callback_demo\",\n",
    "    task_name=\"llm\",\n",
    "    tags=[\"test\"],\n",
    "    # Change the following parameters based on the amount of detail you want tracked\n",
    "    visualize=True,\n",
    "    complexity_metrics=True,\n",
    "    stream_logs=True\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), clearml_callback]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract JSON summarise the article and save results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\n",
    "# wandb_callback = WandbTracer({\"project\": \"wandb_prompts_quickstart\"}\n",
    "# )\n",
    "# callbacks = [StdOutCallbackHandler(), wandb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "json_string = '''```json\n",
    "{\n",
    "   \"category\":\"Visa Business News\",\n",
    "   \"title\":\"Acceptance\",\n",
    "   \"geo\":\"Germany\",\n",
    "   \"audience\":\"Sales\",\n",
    "   \"publication_date\":\"2023-05-04T00:00:00Z\"\n",
    "}\n",
    "```'''\n",
    "json_object = clean_and_load_json(json_string)\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LLM for generating a JSON extract\n",
    "llm_extract_json = build_text_generation_web_ui_client_llm(\n",
    "    parameters={\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.001,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 8192,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "}\n",
    ")\n",
    "\n",
    "# Create an instance of LLM for generating a summary of 200 tokens - Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 5 fields\n",
    "text_extract_json_5 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 3 fields\n",
    "text_extract_json_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract text for summary\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_filepath = '/notebooks/files/extracted_data.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(csv_filepath):\n",
    "    # If it exists, load it into a DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "else:\n",
    "    # If it doesn't exist, initialize an empty DataFrame with the necessary columns\n",
    "    df = pd.DataFrame(columns=[\n",
    "    \"effective-date\", \n",
    "    \"mark-your-calendar-date\", \n",
    "    \"article-id\", \n",
    "    \"category\", \n",
    "    \"title\", \n",
    "    \"geo\", \n",
    "    \"audience\", \n",
    "    \"publication_date\",\n",
    "    \"medium_summary\"\n",
    "    ])\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Define a function to sanitize a file name\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "\n",
    "# Get the first PDF file path from the list\n",
    "for file_path in pdf_files:\n",
    "\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 8 fields\n",
    "    \n",
    "    template_json_8 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input:\n",
    "    1. Article ID\n",
    "    2. Effective date\n",
    "    3. Mark your calendar date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output8_1} the object {output8_2} is a well-formatted instance of the schema.\n",
    "    The object {output8_3} is not well-formatted.\n",
    "    Here is the output schema: {output8_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    ### Sample Input:\n",
    "    Visa Business News\n",
    "    Processing 20 July 2023 File Notification Service Will Be Introduced in Europe\n",
    "    Europe | Acquirers, Issuers, Processors Visa, Plus Networks\n",
    "    Overview: To facilitate faster file collection, beginning Central Processing Date 23 September 2023 Visa will introduce a new BASE II File Notification Service for clients in Europe that use Visa’s Extended Access Servers.\n",
    "    To help improve the BASE II clearing process, beginning Central Processing Date (CPD) 23 September 2023, Visa will start to enable the new File Notification Service (FNS) for clients in Europe markets at no cost. Markets will be enabled on a rolling basis through CPD 4 November 2023 according to the enablement schedule below.\n",
    "    The FNS is designed to improve the BASE II system’s operational performance by\n",
    "    processing successfully staged BASE II files bearing the status of RDYOUT as soon as they are detectable, and will remove the need for clients to customize their BASE II file collection schedules or request ad hoc collection support from Visa.\n",
    "    Mark Your Calendar:\n",
    "    • New FNS will be enabled for clients in Europe (CPD 23 September – CPD 4 November 2023)\n",
    "    Article ID: AI13217\n",
    "\n",
    "    As an example, the new FNS will work as follows:\n",
    "    • Client X has predefined collection times with Visa that are scheduled for 0100 GMT, 1300 GMT and 1430 GMT daily.\n",
    "    • On a particular Monday, Client X stages clearing files at 0110 GMT.\n",
    "    o Under the current service, clearing files would only be collected by Visa at the next available collection time (1300 GMT).\n",
    "    o With the FNS, the EAS will notify BASE II at approximately 0120 GMT that a file has been staged successfully for collection and BASE II will initiate an FNS collection sequence shortly after.\n",
    "    o If Client X stages additional clearing files at 1250 GMT, the current service (scheduled for 1300 GMT) will collect those files instead of the FNS.\n",
    "\n",
    "    ### Sample Response:\n",
    "    {output_sample}\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response: \n",
    "    Valid JSON document that adheres to the schema instance {output8_4}:\n",
    "    ```json\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sub-templates for the chain\n",
    "    output8_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output8_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output8_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output8_4 = '{\"type\": \"object\", \"properties\": {\"category\": {\"type\": \"string\"}, \"title\": {\"type\": \"string\"}, \"geo\": {\"type\": \"string\"}, \"audience\": {\"type\": \"string\"}, \"publication_date\": {\"type\": \"string\", \"format\": \"date\"}, \"article-id\" : {\"type\" : \"string\", \"maxLength\": 7}, \"effective-date\" : {\"type\" : \"string\", \"format\" : \"date\"}, \"mark-your-calendar-date\" : {\"type\" : \"string\", \"format\" : \"date\"}}}'\n",
    "    output_sample = '''\n",
    "    {\n",
    "        \"category\": \"Processing\",\n",
    "        \"title\": \"File Notification Service Will Be Introduced in Europe\",\n",
    "        \"geo\": \"Europe\",\n",
    "        \"audience\": \"Acquirers, Issuers, Processors\",\n",
    "        \"publication_date\": \"2023-07-20\",\n",
    "        \"article-id\": \"AI13217\",\n",
    "        \"effective-date\": \"2023-09-23\",\n",
    "        \"mark-your-calendar-date\": \"2023-09-23\"\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    # Create a prompt\n",
    "    prompt_json_8 = PromptTemplate(template=template_json_8, input_variables=['question', \n",
    "                                                                              'output8_1', \n",
    "                                                                              'output8_2',\n",
    "                                                                              'output8_3',\n",
    "                                                                              'output8_4',\n",
    "                                                                              'output_sample'\n",
    "                                                                              ])\n",
    "\n",
    "    # Create a chain for generating a JSON extract\n",
    "    llm_chain_json_8 = LLMChain(llm=llm_extract_json, prompt=prompt_json_8)\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = document[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 5 fields and load resulting JSON into a variable\n",
    "    json_8 = llm_chain_json_8.run({'question': question, \n",
    "                                   'output8_1': output8_1, \n",
    "                                   'output8_2': output8_2,\n",
    "                                   'output8_3': output8_3,\n",
    "                                   'output8_4': output8_4,\n",
    "                                   'output_sample': output_sample\n",
    "                                   })\n",
    "    \n",
    "    # Print the file path\n",
    "    #print(question)\n",
    "\n",
    "    # Print the results\n",
    "    print(json_8)\n",
    "\n",
    "    json_8 = clean_and_load_json(json_8)\n",
    "\n",
    "    # Split text into chunks for summary generation\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary and load resulting summary into a variable\n",
    "    med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Check if the summary is None and replace it with an empty string\n",
    "    if med_sum is None:\n",
    "        med_sum = ''\n",
    "\n",
    "    # Convert the summary to a string\n",
    "    med_sum = str(med_sum)\n",
    "\n",
    "    # Create a new row for the DataFrame\n",
    "    new_row = pd.DataFrame.from_records([json_8])\n",
    "    new_row['medium_summary'] = [med_sum]\n",
    "    new_row['added'] = datetime.now()\n",
    "\n",
    "    # Add a new row to the existing DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('/notebooks/files/extracted_data.csv', index=False)\n",
    "\n",
    "    def rename_file(df, file_path, fields):\n",
    "        # Get the last row of the DataFrame\n",
    "        row = df.iloc[-1]\n",
    "        \n",
    "        # Initialize an empty list for the new file name components\n",
    "        file_name_components = []\n",
    "        \n",
    "        # For each field...\n",
    "        for field in fields:\n",
    "            # If the field value is not NaN...\n",
    "            if not pd.isna(row[field]):\n",
    "                # Clean the field value and append it to the file name components\n",
    "                file_name_components.append(sanitize_filename(str(row[field])))\n",
    "\n",
    "        # If there are no valid file name components, use the original file name\n",
    "        if not file_name_components:\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            file_name_components.append(base_name)\n",
    "                \n",
    "        # Join the file name components with a dash and add the file extension\n",
    "        new_file_name = '-'.join(file_name_components) + '._pdf'\n",
    "    \n",
    "        # Rename the file\n",
    "        os.rename(file_path, os.path.join(os.path.dirname(file_path), new_file_name))\n",
    "\n",
    "    # Call the function to rename the file at the end of processing each file\n",
    "    rename_file(df, file_path, ['article-id', 'title'])\n",
    "\n",
    "    # Print the new row of a DataFrame\n",
    "    print(new_row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different parameters of the model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/test/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Create an instance of LLM for generating a summary based on a 200 token text Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.18,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter for summary generation\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# Create an empty list to save the results\n",
    "results = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Load PDF file\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #  1. GENERATE MEDIUM SIZE SUMMARY\n",
    "\n",
    "    # Split text into chunks of 2500 characters\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Save results to list\n",
    "    results.append(docs_med_sum)\n",
    "\n",
    "# Print all results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Суммаризация PDF файлов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Суммаризация текста и сохранение результатов в текстовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем путь к папке с PDF-файлами\n",
    "input_directory = \"/notebooks/files\"\n",
    "\n",
    "# Получаем список PDF-файлов\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Создаем инстанс LLM для генерации summary на основе текста размером 200 токенов Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Создаем инстанс разделителя текста на 2500 символов с перекрытием в 0 символов (для генерации summary)\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Создаем цепочку для генерации summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Загружаем PDF-файл\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #1. ГЕНЕРАЦИЯ MEDIAN SIZE SUMMARY\n",
    "\n",
    "    #Разрезаем текст на куски по 2500 символов \n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "    # Отправляем документы в обработку для генерации summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Заменяем расширение файла на .txt и сохраняем результат\n",
    "    output_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        output_file.write(docs_med_sum)\n",
    "\n",
    "    print(f\"Обработан файл: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем PDF документ и делаем из него текстовый файл с помощью langchain.document_loaders.PDFMinerLoader\n",
    "file_path = '/notebooks/files/AI12944 - Updates to Fraud and Consumer Dispute Rules.pdf'\n",
    "loader = PDFMinerLoader(file_path)\n",
    "document = loader.load()\n",
    "\n",
    "text = document[0].page_content\n",
    "\n",
    "llm = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.001,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 1,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "    })\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose = True)\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
