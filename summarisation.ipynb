{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "# import wandb\n",
    "from datetime import datetime\n",
    "# from wandb.integration.langchain import WandbTracer\n",
    "from langchain.callbacks import ClearMLCallbackHandler\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, AgentExecutor, BaseSingleActionAgent, AgentType\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "os.chdir(\"/notebooks/learn-langchain\")\n",
    "from langchain_app.models.text_generation_web_ui import (\n",
    "    build_text_generation_web_ui_client_llm,\n",
    ")\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# from langchain.callbacks import WandbCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# Set Wandb API key\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"9ae105f6bbe37ca6eff03ea9c3af7df398713e7e\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ctransformers and Falcon-40B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "\n",
    "config = {'max_new_tokens': 50, 'repetition_penalty': 1.1}\n",
    "\n",
    "llm = CTransformers(model='/notebooks/text-generation-webui/models/mpt-30B-instruct-GGML/mpt-30b-instruct.ggmlv0.q4_1.bin', model_type='mpt', gpu_layers=50, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm('AI is going to'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract JSON summarise the article and save results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "json_string = '''```json\n",
    "{\n",
    "   \"category\":\"Visa Business News\",\n",
    "   \"title\":\"Acceptance\",\n",
    "   \"geo\":\"Germany\",\n",
    "   \"audience\":\"Sales\",\n",
    "   \"publication_date\":\"2023-05-04T00:00:00Z\"\n",
    "}\n",
    "```'''\n",
    "json_object = clean_and_load_json(json_string)\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LLM for generating a JSON extract\n",
    "llm_extract_json = build_text_generation_web_ui_client_llm(\n",
    "    parameters={\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "}\n",
    ")\n",
    "\n",
    "# Create an instance of LLM for generating a summary of 200 tokens - Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 5 fields\n",
    "text_extract_json_5 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 3 fields\n",
    "text_extract_json_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract text for summary\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_filepath = '/notebooks/files/extracted_data.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(csv_filepath):\n",
    "    # If it exists, load it into a DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "else:\n",
    "    # If it doesn't exist, initialize an empty DataFrame with the necessary columns\n",
    "    df = pd.DataFrame(columns=[\n",
    "    \"effective-date\", \n",
    "    \"mark-your-calendar-date\", \n",
    "    \"article-id\", \n",
    "    \"category\", \n",
    "    \"title\", \n",
    "    \"geo\", \n",
    "    \"audience\", \n",
    "    \"publication_date\",\n",
    "    \"medium_summary\"\n",
    "    ])\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Define a function to sanitize a file name\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "\n",
    "# Get the first PDF file path from the list\n",
    "for file_path in pdf_files:\n",
    "\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 3 fields\n",
    "    \n",
    "    template_json_3 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input:\n",
    "    1. Article ID\n",
    "    2. Effective date\n",
    "    3. Mark your calendar date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output3_1} the object {output3_2} is a well-formatted instance of the schema.\n",
    "    The object {output3_3} is not well-formatted.\n",
    "    Here is the output schema: {output3_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    As an example, this text:\n",
    "\n",
    "    As published in the DCC Guide—DCC Program Requirements (a Visa Supplemental Requirement; see Additional Resources), acquirers registered in the DCC Compliance Program must register their DCC-enabled merchants with Visa annually. Merchant registration information for the period ending 31 July 2023 is due 15 August 2023. This requirement applies to merchants offering DCC in either a card-present or card-not-present environment. \n",
    "    Article ID: AI13088\n",
    "    Also effective 8 June 2023 edition of the Visa Business News, Visa announced updates to the ATM Locator Update System, including an updated ATM locator template,\n",
    "\n",
    "    Mark Your Calendar:\n",
    "    • DeadlineforDCCCompliance Program annual merchant registration (15 August 2023)\n",
    "    • Deadlinetoderegisterasan acquirer from the DCC program prior to fiscal year 2024 billing (30 September 2023)\n",
    "\n",
    "    Acquirers will receive an email confirming receipt of their submission and will only be contacted in the event of a problem with their registration. Further details about the DCC merchant registration process can be found in the DCC Guide—DCC Program Requirements.\n",
    "\n",
    "    Results in the json:\n",
    "        \n",
    "    \"article-id\": \"AI13088\",\n",
    "    \"effective-date\": \"2023-08-15\",\n",
    "    \"mark-your-calendar-date\": \"2023-06-08\"\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response: \n",
    "    Valid JSON document that adheres to the schema instance: ```json\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 5 fields\n",
    "    template_json_5 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input.\n",
    "    1. Category\n",
    "    2. Publication date\n",
    "    3. Article title\n",
    "    4. GEO\n",
    "    5. Article audience\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output5_1} the object {output5_2} is a well-formatted instance of the schema.\n",
    "    The object {output5_3} is not well-formatted.\n",
    "    Here is the output schema: {output5_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    As an example, this text:\n",
    "    Visa Business News\n",
    "    Risk Products 1 June 2023 Card-Not-Present Token Risk Management Tools and Best Practices\n",
    "    Europe | Acquirers, Issuers, Processors Visa Network; Europe Processing\n",
    "    Overview: Visa is reminding clients of the tools and best practices available for managing token provisioning and transaction risk for the four main token use cases in the card-not-present environment.\n",
    "\n",
    "    Results in the json:\n",
    "    \n",
    "    \"category\": \"Risk Products\",\n",
    "    \"publication_date\": \"2023-06-01\",\n",
    "    \"title\": \"Card-Not-Present Token Risk Management Tools and Best Practices\",\n",
    "    \"geo\": \"Europe\",\n",
    "    \"audience\": \"Acquirers, Issuers, Processors\"\n",
    "    \n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    Valid JSON document that adheres to the schema instance: ```json\n",
    "    \"\"\"\n",
    "\n",
    "    template_eval_json = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Format a json string in Input.\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "\n",
    "    Here is the output schema: {output}\n",
    "\n",
    "    For example, this invalid JSON:\n",
    "    {wrong_json}\n",
    "\n",
    "    After formatting valid JSON:\n",
    "    {correct_json}\n",
    "\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "    ### Input:\n",
    "    {question}\n",
    "\n",
    "    ### Response: \n",
    "    Valid JSON document that adheres to the schema instance: ```json\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sub-templates for the chain\n",
    "    output3_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output3_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output3_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output3_4 = '{\"type\" : \"object\", \"properties\" : {\"article-id\" : {\"type\" : \"string\", \"maxLength\": 7}, \"effective-date\" : {\"type\" : \"string\", \"format\" : \"date\"}, \"mark-your-calendar-date\" : {\"type\" : \"string\", \"format\" : \"date\"}}}'\n",
    "    output5_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output5_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output5_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output5_4 = '{\"type\": \"object\", \"properties\": {\"category\": {\"type\": \"string\"}, \"title\": {\"type\": \"string\"}, \"geo\": {\"type\": \"string\"}, \"audience\": {\"type\": \"string\"}, \"publication_date\": {\"type\": \"string\", \"format\": \"date\"}}}'\n",
    "    wrong_json_3 = \"\"\"\n",
    "    {\n",
    "            \"article-id\": \"AI13012\"\n",
    "            \"effective_date\": \"27/5/23\",\n",
    "            \"mark_your_calendar_dates\": [\n",
    "                \"VBS login page user experience change (27 May 23)\",\n",
    "                \"MFA enablement window for all VBS users (25 Jun 23)\"\n",
    "            ]\n",
    "        }\n",
    "    ```\n",
    "    \"\"\"\n",
    "    correct_json_3 = \"\"\"\n",
    "    {\n",
    "        \"audience\": \"Issuers\",\n",
    "        \"article-id\": \"AI13012\",\n",
    "        \"effective-date\": \"2023-05-27\",\n",
    "        \"mark_your_calendar_date\": [\"VBS login page user experience change (2023-05-27)\", \"MFA enablement window for all VBS users (2023-06-25)\"]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    wrong_json_5 = \"\"\"\n",
    "    {\n",
    "            \"category\": \"Commercial Solutions\",\n",
    "            \"title\": \"Multi-Factor Authentication Will Be Enabled in June 2023 to Protect Visa Business Solutions Services\",\n",
    "            \"geo\": \"Global\"\n",
    "            \"audience\": \"Issuers\",\n",
    "            \"publication_date\": \"25/6/23\",\n",
    "        }\n",
    "    ```\n",
    "    \"\"\"\n",
    "    correct_json_5 = \"\"\"\n",
    "    {\n",
    "        \"category\": \"Commercial Solutions\",\n",
    "        \"title\": \"Multi-Factor Authentication Will Be Enabled in June 2023 to Protect Visa Business Solutions Services\",\n",
    "        \"geo\": \"Global\",\n",
    "        \"audience\": \"Issuers\",\n",
    "        \"publication_date\": \"2023-06-25\"\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt_json_5 = PromptTemplate(template=template_json_5, input_variables=['question', \n",
    "                                                                              'output5_1', \n",
    "                                                                              'output5_2',\n",
    "                                                                              'output5_3',\n",
    "                                                                              'output5_4'\n",
    "                                                                              ])\n",
    "    prompt_json_3 = PromptTemplate(template=template_json_3, input_variables=['question', \n",
    "                                                                              'output3_1', \n",
    "                                                                              'output3_2',\n",
    "                                                                              'output3_3',\n",
    "                                                                              'output3_4'\n",
    "                                                                              ])\n",
    "    prompt_eval_json = PromptTemplate(template=template_eval_json, input_variables=['question',\n",
    "                                                                                      'output',\n",
    "                                                                                      'wrong_json',\n",
    "                                                                                      'correct_json'\n",
    "                                                                                      ])\n",
    "\n",
    "\n",
    "    # Create a chain for generating a JSON extract\n",
    "    llm_chain_json_5 = LLMChain(llm=llm, prompt=prompt_json_5)\n",
    "    llm_chain_json_3 = LLMChain(llm=llm, prompt=prompt_json_3)\n",
    "    llm_chain_eval_json = LLMChain(llm=llm, prompt=prompt_eval_json)\n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 5 fields\n",
    "    docs_json_5 = text_extract_json_5.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_5[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 5 fields and load resulting JSON into a variable\n",
    "    json_5 = llm_chain_json_5.run({'question': question, \n",
    "                                   'output5_1': output5_1, \n",
    "                                   'output5_2': output5_2,\n",
    "                                   'output5_3': output5_3,\n",
    "                                   'output5_4': output5_4\n",
    "                                   })\n",
    "    json_5 = llm_chain_eval_json.run({'question': json_5,\n",
    "                                      'output': output5_4,\n",
    "                                      'wrong_json': wrong_json_5,\n",
    "                                      'correct_json': correct_json_5\n",
    "                                     })\n",
    "    \n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 3 fields\n",
    "    docs_json_3 = text_extract_json_3.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_3[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 3 fields and load resulting JSON into a variable\n",
    "    json_3 = llm_chain_json_3.run({'question': question, \n",
    "                                   'output3_1': output3_1, \n",
    "                                   'output3_2': output3_2,\n",
    "                                   'output3_3': output3_3,\n",
    "                                   'output3_4': output3_4\n",
    "                                  })\n",
    "    \n",
    "    json_3 = llm_chain_eval_json.run({'question': json_3,\n",
    "                                        'output': output3_4,\n",
    "                                        'wrong_json': wrong_json_3,\n",
    "                                        'correct_json': correct_json_3\n",
    "                                         })\n",
    "    \n",
    "    # Print the file path\n",
    "    print(file_path)\n",
    "\n",
    "    # Print the results\n",
    "    print(json_5, json_3)\n",
    "\n",
    "    json_3 = clean_and_load_json(json_3)\n",
    "    json_5 = clean_and_load_json(json_5)\n",
    "    # Combine two JSONs into one\n",
    "    combined_json = {**json_3, **json_5}\n",
    "\n",
    "    # Split text into chunks for summary generation\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary and load resulting summary into a variable\n",
    "    med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Check if the summary is None and replace it with an empty string\n",
    "    if med_sum is None:\n",
    "        med_sum = ''\n",
    "\n",
    "    # Convert the summary to a string\n",
    "    med_sum = str(med_sum)\n",
    "\n",
    "    # Create a new row for the DataFrame\n",
    "    new_row = pd.DataFrame.from_records([combined_json])\n",
    "    new_row['medium_summary'] = [med_sum]\n",
    "    new_row['added'] = datetime.now()\n",
    "\n",
    "    # Add a new row to the existing DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('/notebooks/files/extracted_data.csv', index=False)\n",
    "\n",
    "    def rename_file(df, file_path, fields):\n",
    "        # Get the last row of the DataFrame\n",
    "        row = df.iloc[-1]\n",
    "        \n",
    "        # Initialize an empty list for the new file name components\n",
    "        file_name_components = []\n",
    "        \n",
    "        # For each field...\n",
    "        for field in fields:\n",
    "            # If the field value is not NaN...\n",
    "            if not pd.isna(row[field]):\n",
    "                # Clean the field value and append it to the file name components\n",
    "                file_name_components.append(sanitize_filename(str(row[field])))\n",
    "\n",
    "        # If there are no valid file name components, use the original file name\n",
    "        if not file_name_components:\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            file_name_components.append(base_name)\n",
    "                \n",
    "        # Join the file name components with a dash and add the file extension\n",
    "        new_file_name = '-'.join(file_name_components) + '._pdf'\n",
    "    \n",
    "        # Rename the file\n",
    "        os.rename(file_path, os.path.join(os.path.dirname(file_path), new_file_name))\n",
    "\n",
    "    # Call the function to rename the file at the end of processing each file\n",
    "    rename_file(df, file_path, ['article-id', 'title'])\n",
    "\n",
    "    # Print the new row of a DataFrame\n",
    "    print(new_row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Output parser Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.chains import LLMChain, TransformChain, SequentialChain\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from datetime import date\n",
    "\n",
    "# Create an instance of LLM for generating a JSON extract\n",
    "llm_extract_json = build_text_generation_web_ui_client_llm(\n",
    "    parameters={\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 8192,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "}\n",
    ")\n",
    "\n",
    "model = llm_extract_json\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Article(BaseModel):\n",
    "    article_id: Optional[str] = Field(None, max_length=7)\n",
    "    effective_date: Optional[str]\n",
    "    mark_your_calendar_date: Optional[str]\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Article)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "article_query = \"\"\" \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input:\n",
    "    1. Article ID\n",
    "    2. Effective date\n",
    "    3. Mark your calendar date\n",
    "\n",
    "    ### Input:    \n",
    "\n",
    "    Visa Business News\n",
    "    Acceptance\n",
    "    DCC Compliance Program: Annual DCC-Enabled Merchant / ATM Registration Requirements\n",
    "    Global | Acquirers, Processors, Agents Visa, Plus Networks; V PAY\n",
    "    8 June 2023\n",
    "    Overview: Acquirers registered in the Dynamic Currency Conversion (DCC) Compliance Program for POS, e-commerce, and/or ATM must register their DCC-enabled merchants for the period ending 31 July 2023 by 15 August 2023 and must update the ATM Locator quarterly with their DCC-enabled ATMs. Acquirers must advise Visa by 30 September 2023 if they wish to deregister from the DCC Compliance Program for fiscal year 2024.\n",
    "    As published in the DCC Guide—DCC Program Requirements (a Visa Supplemental Requirement; see Additional Resources), acquirers registered in the DCC Compliance Program must register their DCC-enabled merchants with Visa annually. Merchant registration information for the period ending 31 July 2023 is due 15 August 2023. This requirement applies to merchants offering DCC in either a card-present or card-not-present environment. Acquirers registered in the DCC Compliance Program for ATMs must provide a list of ATM locations enabled for DCC every quarter through the existing Visa ATM Locator update process.\n",
    "    Providing Accurate Information\n",
    "    Merchant / ATM registration details are used for DCC audit1 purposes, so acquirers must verify their accuracy. Inaccurate data, including the registration of merchants or ATMs that are not DCC-enabled, may result in non-compliance assessments (NCAs).\n",
    "    For DCC-enabled merchants, the merchant name field in the merchant registration record must match the merchant name field in the BASE II clearing record, including spaces (as applicable). In addition, each merchant record must contain the exact street address of the merchant’s physical location where DCC is offered. Corporate head office addresses and/or post office box numbers are not acceptable. E-commerce merchants must list their web address.\n",
    "    For quarterly ATM Locator updates, the batch file template must be updated with “Y” in column L (DYNAMIC CUR CONV) to indicate if the ATM is DCC-enabled.\n",
    "    How to Register DCC Merchants and ATMs\n",
    "    All DCC-registered acquirers that have successfully certified their DCC solutions with Visa must submit a complete list of their DCC-enabled merchants as of 31 July 2023 by 15 August 2023. Merchant records must be submitted\n",
    "    Mark Your Calendar:\n",
    "    • DeadlineforDCCCompliance Program annual merchant registration (15 August 2023)\n",
    "    • Deadlinetoderegisterasan acquirer from the DCC program prior to fiscal year 2024 billing (30 September 2023)\n",
    "    Article ID: AI13088\n",
    "    to Visa according to the Merchant Registration Data Elements document (see Additional Resources) and emailed to DCCProgram@visa.com with “Acquirer Name (Country) - Merchant Registration” in the subject line.\n",
    "    Acquirers that do not submit their merchant registration list by 15 August 2023 may be subject to NCAs. Acquirers will receive an email confirming receipt of their submission and will only be contacted in the event of a problem with their registration. Further details about the DCC merchant registration process can be found in the DCC Guide—DCC Program Requirements.\n",
    "    Note: The DCC Guide—DCC Program Requirements applies to clients in all Visa regions.\n",
    "    Also in the 8 June 2023 edition of the Visa Business News, Visa announced updates to the ATM Locator Update System, including an updated ATM locator template, which will take effect 2 July 2023. Refer to this article in Additional Resources for details about these updates to the quarterly ATM Locator update process.\n",
    "    Acquirer Deregistration from the DCC Compliance Program\n",
    "    If an acquirer wishes to stop offering DCC at its merchants or ATMs, it must deregister by sending an email to DCCProgram@visa.com with the date by which it plans to stop offering DCC.\n",
    "    For the upcoming DCC program fee billing period, 1 October 2023 – 30 September 2024, acquirers must notify Visa of their intent to deregister from the DCC Compliance Program no later than 30 September 2023 to avoid assessment of the annual fee.\n",
    "    1 Visa may, at its discretion, audit physical merchant locations, e-commerce merchants and ATMs to confirm compliance with the DCC rules. A DCC audit will seek to determine if the transaction is compliant with Visa disclosure, active choice, transaction receipt and DCC indicator requirements. Acquirers will be notified of the results of the audit and of any corrective action required or NCAs that may apply.\n",
    "        Additional Resources\n",
    "        Documents & Publications\n",
    "    “New Global ATM Locator Update System,” Visa Business News, 8 June 2023\n",
    "    Online Resources\n",
    "    Refer to the Dynamic Currency Conversion page at Visa Online for more information and the following documentation:\n",
    "    • DCC Guide—DCC Program Requirements\n",
    "    • Merchant Registration Data Elements\n",
    "    Visit the Visa ATM Solutions Resources page at Visa Online to view the Visa Global ATM Locator Update System Training document.\n",
    "    Note: For Visa Online resources, you will be prompted to log in.\n",
    "        For More Information\n",
    "        AP, CEMEA, LAC: Contact your Visa representative or email DCCProgram@visa.com. Canada and U.S.: Contact eSupport@visa.com.\n",
    "    Article ID: AI13088\n",
    "\n",
    "    \n",
    "    Europe: Contact Visa customer support on your country-specific number, or email CustomerSupport@visa.com or DCCProgram@visa.com.\n",
    "    Third party agents: Contact your acquirer, processor or Visa representative.\n",
    "        Notice: This Visa communication is furnished to you solely in your capacity as a customer of Visa Inc. (through its operating companies of Visa U.S.A Inc., Visa International Service Association, Visa Worldwide Pte. Ltd, Visa Europe Ltd., Visa International Servicios de Pago España, S.R.L.U. and Visa Canada Corporation) or its authorized agent, or as a participant in the Visa payments system. By accepting this Visa communication, you acknowledge that the information contained herein (the \"Information\") is confidential and subject to the confidentiality restrictions contained in the Visa Rules, which limit your use of the Information. You agree to keep the Information confidential and not to use the Information for any purpose other than in your capacity as a customer of Visa Inc. or as a participant in the Visa payments system. You may disseminate this Information to a merchant participating in the Visa payments system if: (i) you serve the role of “acquirer” within the Visa payments system; (ii) you have a direct relationship with such merchant which includes an obligation to keep Information confidential; and (iii) the Information is designated as “affects merchants” demonstrated by display of the storefront icon on the communication. A merchant receiving such Information must maintain the confidentiality of such Information and disseminate and use it on a “need to know” basis and only in their capacity as a participant in the Visa payments system. Except as otherwise provided, the Information may only be disseminated within your organization on a need-to-know basis to enable your participation in the Visa payments system. Visa is not responsible for errors in or omissions from this publication.\n",
    "    Article ID: AI13088\n",
    "\n",
    "    ### Response:\n",
    "\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_key=\"json_string\",\n",
    ")\n",
    "\n",
    "def parse_output(inputs: dict) -> dict:\n",
    "    text = inputs[\"json_string\"]\n",
    "    return {\"result\": parser.parse(text)}\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"json_string\"],\n",
    "    output_variables=[\"result\"],\n",
    "    transform=parse_output\n",
    ")\n",
    "\n",
    "chain = SequentialChain(\n",
    "    input_variables=[\"query\"],\n",
    "    output_variables=[\"result\"],\n",
    "    chains=[llm_chain, transform_chain],\n",
    ")\n",
    "\n",
    "result = chain.run(query=article_query)\n",
    "parsed_output = result[\"result\"]\n",
    "\n",
    "# Теперь parsed_output - это экземпляр класса Article, который можно сериализовать в JSON.\n",
    "# json_output = parsed_output.json()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different parameters of the model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/test/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Create an instance of LLM for generating a summary based on a 200 token text Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.18,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter for summary generation\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# Create an empty list to save the results\n",
    "results = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Load PDF file\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #  1. GENERATE MEDIUM SIZE SUMMARY\n",
    "\n",
    "    # Split text into chunks of 2500 characters\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Save results to list\n",
    "    results.append(docs_med_sum)\n",
    "\n",
    "# Print all results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Суммаризация PDF файлов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Суммаризация текста и сохранение результатов в текстовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/root/miniconda3/envs/textgen/bin/python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/test\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Create an instance of LLM for generating a summary based on a 200 token text Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 250,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter for summary generation\n",
    "text_splitter_med_sum = SpacyTextSplitter(\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "# Start processing files\n",
    "for file_path in pdf_files:\n",
    "    # Load PDF file\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    # GENERATE MEDIUM SIZE SUMMARY\n",
    "\n",
    "    # Split text into chunks\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Replace the file extension with .txt and save the result\n",
    "    output_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        output_file.write(docs_med_sum)\n",
    "\n",
    "    print(f\"Обработан файл: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем PDF документ и делаем из него текстовый файл с помощью langchain.document_loaders.PDFMinerLoader\n",
    "file_path = '/notebooks/files/AI12944 - Updates to Fraud and Consumer Dispute Rules.pdf'\n",
    "loader = PDFMinerLoader(file_path)\n",
    "document = loader.load()\n",
    "\n",
    "text = document[0].page_content\n",
    "\n",
    "llm = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.001,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 1,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "    })\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose = True)\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
